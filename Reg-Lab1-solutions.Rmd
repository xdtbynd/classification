---
title: "ASML Regression, Computer Lab 1"
author: "Jochen Einbeck"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## durhamSLR package
If you have not already done so, install the durhamSLR package by uncommenting and executing the R expression below
```{r}
# Uncomment and run if package is not yet installed
#install.packages('durhamSLR', repos = c('https://nseg4.r-universe.dev', 'https://cloud.r-project.org'))
```


## Real estate prize data: Preliminaries

We consider a data set on real estate prizes extracted from  [kaggle](https://www.kaggle.com/quantbruce/real-estate-price-prediction/).

You can read in the data via

```{r}
realestate<- read.csv("http://www.maths.dur.ac.uk/~dma0je/Data/Realestate.csv", header=TRUE)
```

Carry out some simple commands to visualize the data and their structure, such as

```{r}
dim(realestate)
colnames(realestate)
pairs(realestate)
hist(realestate$Y.house.price.of.unit.area)
boxplot(realestate[,2:8])
```

Let us do the following simplifications to avoid long variable names:

```{r}
colnames(realestate)[8]<- "Y"
colnames(realestate)[5]<- "X4.stores"
colnames(realestate)[4] <- "X3.MRT"
```


## Simple linear regression

We consider firstly a simple linear regression model of the response, $Y$, against variable $\mbox{X3.MRT}$.  Carry out the corresponding least squares regression using `lm`, and save the fitted model into an  object `real.fit.l2`. Create a scatterplot of the two variables, and add the fitted regression line. Display also the `summary` of the fitted model.


```{r}
real.fit.l2<- lm(Y~ X3.MRT, data=realestate)

plot(realestate$X3.MRT, realestate$Y)
abline(real.fit.l2, lty=1, col=2, lwd=2)

summary(real.fit.l2)

```


You see that in the second colum of this table we have the *standard errors*, $SE(\hat{\beta}_j)$.  For the L2 linear regression model, we are in the fortunate situation that we can calculate these analytically. However, for other models or methods this is not necessarily the case. 

However, standard errors for most problems can be obtained through the *bootstrap*.  The idea of this is roughly as follows:

* From the original data $(\boldsymbol{x}_i, y_i)$, resample $B$ data sets of size $n$ (with replacement).
* To each of these $B$ data sets, fit the model of interest, and record the parameters of interest for each fitted model.
* Calculate the standard deviations of the $B$ parameter estimates in order to obtain an estimate of $SE(\hat{\beta}_j)$.

This (so called *non-parametric* bootstrap) is implemented in the below.  (In the *parametric* bootstrap, the first step would be replaced by sampling new responses from the fitted model.) Make it clear to you what happens in this code; and execute it.


```{r}
B <- 199
n <- dim(realestate)[1]
Ynew <- matrix(0, n, B)
X <- model.matrix(real.fit.l2)
Y<- realestate$Y
Xnew<-list()
for (b in 1:B){
  Xnew[[b]]<- X
  for (i in 1:n){
    j<-sample(n,1)
    Xnew[[b]][i,]<- X[j,]
    Ynew[i,b] <-Y[j]
  }
}

all.real.boot <-matrix(0, B, 2)

for (b in 1:B){
  real.fitb<- lm(Ynew[,b]~Xnew[[b]])
  all.real.boot[b,]<- summary(real.fitb)$coef[,1]
}

all.boot1.sd<- apply(all.real.boot, 2, sd)
```


Produce a matrix which in the first row contains the standard errors of the linear model fitted through the `lm` function, and in the second row the standard errors obtained through the Bootstrap procedure.

```{r}
both.real.sd <- rbind( summary(real.fit.l2)$coef[,2], all.boot1.sd)
both.real.sd
```

A second approach to carry out Uncertainty Quantification in a principled manner is the Bayesian route.
Let us now consider a Bayesian version of the simple linear regression model. We can do this as in the lecture using `rjags`.  To set up the model, you can use exactly the code as provided on the lecture slides:

```{r}
model1_string <- "model{
  for(i in 1:N){ 
    y[i] ~ dnorm(mu[i], tau)   # tau = precision=inverse variance
    mu[i] <- beta0+beta1*x[i]
  } 
  # Prior distribution on mean
    beta0 ~ dnorm(0, 0.0001)    
    beta1~  dnorm(0, 0.0001)
    
    tau  ~ dgamma(0.01, 0.01)
    sigma<- 1/sqrt(tau)
   
}"
```
 

The part which needs to be modified, as compared to the lecture, is the `data` argument of the `jags.model` call; everything else stays the same. At the end of the process, check the convergence and mixing of the sampler and produce the `summary' output of the fitted model. Compare the parameter estimates and standard errors with those obtained previously.
 
```{r}
require(rjags)
require(durhamSLR)
nchains = 4
model1 <- jags.model(textConnection(model1_string), 
    data = list(x=realestate$X3.MRT, y=realestate$Y,N=length(realestate$Y)), n.chains=nchains
    )
update(model1, 10000)

postmod1.samples = coda.samples(model1, c("beta0", "beta1", "sigma"), 10000)

# Extract MCMC draws as tabular output
tabular_mcmc = array(NA, c(nrow(postmod1.samples[[1]]), nchains, ncol(postmod1.samples[[1]])))
dimnames(tabular_mcmc) = list(iterations=NULL, chains=paste("chain:", 1:nchains, sep=""),
parameters=colnames(postmod1.samples[[1]]))
for(i in 1:nchains) {
  tabular_mcmc[,i,] = as.matrix(postmod1.samples[[i]])
}
# Assess convergence and mixing:
diagnostics(tabular_mcmc)
# Generate posterior summaries:
summary(postmod1.samples)
```


You see that Jags has also delivered us confidence intervals. Such confidence intervals are also available from the standard linear model fit, as well as from the bootstrap. For the standard linear model, these can be obtained through `confint`, and for the bootstrap by applying the `quantile` function on the bootstrap samples. Obtain such intervals at the $95%$ level of confidence, and report and compare the results of the three approaches.

```{r}
# From linear model (analytic)
confint(real.fit.l2)
# Bootstrap
quantile(all.real.boot[,1],probs=c(0.025, 0.975) )
quantile(all.real.boot[,2],probs=c(0.025, 0.975) )
```

## LAD regression

We will now carry out a replication of the work above, but using L1 instead of L2 regression. To begin with, fit the same simple linear model as above, this time using function `rq`, and then inspect the `summary` of the fitted object.

```{r}
require(quantreg)
real.fit.l1 <- rq(Y~X3.MRT, data=realestate)
summary(real.fit.l1)
```

Overlay the L1 regression fit to the scatterplot produced above, adding a legend in order to clearly identify the L1 and L2 lines.

```{r}
plot(realestate$X3.MRT, realestate$Y)
abline(real.fit.l2, lty=1, col=2, lwd=2)
abline(real.fit.l1, lty=1, col=4, lwd=2)
legend(4000,100, c("L2", "L1"),lwd=c(2,2), col=c(2,4))
```


Unfortunately, there is now way of extracting standard errors from an `rq` object, and also
the standard bootstrap method, as introduced above, will fail for L1 (or quantile) regression due to computational problems. Some bootstrap variants tailored to the L1/quantile regression problem have been developed, see `?boot.rq`.  The easiest way of accessing the L1 bootstrap method is through the `summ` functionality of the `jtools` package: 

```{r}
require(jtools)
summ(real.fit.l1, se="boot")
```

In order to produce our Bayesian analysis with Jags, the likelihood (for `y[i]`) is now Laplace (also called double-exponential) rather than Gaussian, denoted `ddexp`.  Otherwise you can leave things as they are.

```{r}
model3_string <- "model{
  for(i in 1:N){ 
    y[i] ~ ddexp(mu[i], tau)   # tau = precision=inverse variance
    mu[i] <- beta0+beta1*x[i]
  } 
  # Prior distribution on mean
    beta0 ~ dnorm(0, 0.0001)    
    beta1~  dnorm(0, 0.0001)
    
    tau  ~ dgamma(0.01, 0.01)
    sigma<- 1/sqrt(tau)
   
}"
```


```{r}
nchains = 4
model3 <- jags.model(textConnection(model3_string), 
    data = list(x=realestate$X3.MRT, y=realestate$Y,N=length(realestate$Y)), n.chains=nchains
    )

update(model3, 10000)

postmod3.samples = coda.samples(model3, c("beta0", "beta1", "tau"), 10000)
```

Again, check the convergence and mixing of the sampler. Then produce the summary output from the posterior samples, and compare their means and standard deviations with their frequentist L1 counterparts:

```{r}
# Extract MCMC draws as tabular output
tabular_mcmc3 = array(NA, c(nrow(postmod3.samples[[1]]), nchains, ncol(postmod3.samples[[1]])))
dimnames(tabular_mcmc3) = list(iterations=NULL, chains=paste("chain:", 1:nchains, sep=""),
parameters=colnames(postmod3.samples[[1]]))
for(i in 1:nchains) {
  tabular_mcmc3[,i,] = as.matrix(postmod3.samples[[i]])
}
# Assess convergence and mixing:
diagnostics(tabular_mcmc3)
# Generate posterior summaries:
summary(postmod3.samples)
# Compare means
rbind(apply(tabular_mcmc3[,,c("beta0", "beta1")], 3, mean),
      real.fit.l1$coef[1:2])
# Compare standard deviations
rbind(round(apply(tabular_mcmc3[,,c("beta0", "beta1")], 3, sd), digits=4),
      round(summ(real.fit.l1, se="boot")$coef[1:2, 2], digits=4))
```


